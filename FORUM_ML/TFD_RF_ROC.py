import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV
from sklearn.metrics import accuracy_score, log_loss
from scipy.stats import randint
from sklearn.metrics import roc_curve, auc




# üîπ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df = pd.read_csv("C:\\Users\\surfa\\OneDrive\\Desktop\\Forum\\Data\\data_augmented_TFD_balanced_350.csv")

# üîπ ‡πÅ‡∏¢‡∏Å Features ‡πÅ‡∏•‡∏∞ Target
X = df.drop('TFD', axis=1)
y = df['TFD']

# üîπ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Train/Test (80% Train, 20% Test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)

# üîπ ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏°
param_dist = {
    'n_estimators': randint(50, 200),  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ
    'max_depth': [None, 10, 20, 30, 40],  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ
    'min_samples_split': [2, 5, 10],  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡πà‡∏á node
    'min_samples_leaf': [1, 2, 4],  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡πÉ‡∏ô‡πÉ‡∏ö‡πÑ‡∏°‡πâ
    'max_features': ['sqrt', 'log2'],  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô feature ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÅ‡∏¢‡∏Å node
    'bootstrap': [True, False]  # ‡πÉ‡∏ä‡πâ bootstrap ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
}

# üîπ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• RandomForest
rf = RandomForestClassifier(random_state=42, n_jobs=-1)

# üîπ ‡πÉ‡∏ä‡πâ RandomizedSearchCV ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏∏‡πà‡∏°‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
random_search = RandomizedSearchCV(rf, param_distributions=param_dist, 
                                   n_iter=20, cv=5, scoring='accuracy', 
                                   random_state=42, n_jobs=-1, verbose=1)

# üîπ ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
random_search.fit(X_train, y_train)

# üîπ ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å RandomizedSearchCV
best_params_random = random_search.best_params_
print("Best Parameters from Randomized Search:", best_params_random)
print("Best Accuracy from Randomized Search:", random_search.best_score_)

# üîπ ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å RandomizedSearchCV ‡∏°‡∏≤ Fine-Tune ‡∏î‡πâ‡∏ß‡∏¢ GridSearchCV
param_grid = {
    'n_estimators': [best_params_random['n_estimators'] - 25, best_params_random['n_estimators'], best_params_random['n_estimators'] + 25],
    'max_depth': [best_params_random['max_depth'] - 10 if best_params_random['max_depth'] else None, best_params_random['max_depth'], best_params_random['max_depth'] + 10 if best_params_random['max_depth'] else None],
    'min_samples_split': [max(2, best_params_random['min_samples_split'] - 1), best_params_random['min_samples_split'], best_params_random['min_samples_split'] + 1],
    'min_samples_leaf': [max(1, best_params_random['min_samples_leaf'] - 1), best_params_random['min_samples_leaf'], best_params_random['min_samples_leaf'] + 1],
    'max_features': [best_params_random['max_features']],
    'bootstrap': [best_params_random['bootstrap']]
}

# üîπ ‡πÉ‡∏ä‡πâ GridSearchCV ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)

# üîπ ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
grid_search.fit(X_train, y_train)

# üîπ ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å GridSearchCV
best_params_grid = grid_search.best_params_
print("Best Parameters from Grid Search:", best_params_grid)
print("Best Accuracy from Grid Search:", grid_search.best_score_)

# üîπ ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
final_model = RandomForestClassifier(**best_params_grid, random_state=42, n_jobs=-1)
final_model.fit(X_train, y_train)

# üîπ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
y_pred_train = final_model.predict(X_train)
y_pred_test = final_model.predict(X_test)

# üîπ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Accuracy ‡πÅ‡∏•‡∏∞ Log Loss
train_acc = accuracy_score(y_train, y_pred_train)
test_acc = accuracy_score(y_test, y_pred_test)
train_loss = log_loss(y_train, final_model.predict_proba(X_train))
test_loss = log_loss(y_test, final_model.predict_proba(X_test))

# üîπ ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
print(f"\n‚úÖ Final Train Accuracy: {train_acc:.4f}")
print(f"‚úÖ Final Test Accuracy: {test_acc:.4f}")
print(f"‚úÖ Final Train Log Loss: {train_loss:.4f}")
print(f"‚úÖ Final Test Log Loss: {test_loss:.4f}")

# üîπ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™ 1 (TFD = 1)
y_prob_train = final_model.predict_proba(X_train)[:, 1]
y_prob_test = final_model.predict_proba(X_test)[:, 1]

# üîπ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ FPR, TPR ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Train ‡πÅ‡∏•‡∏∞ Test
fpr_train, tpr_train, _ = roc_curve(y_train, y_prob_train)
fpr_test, tpr_test, _ = roc_curve(y_test, y_prob_test)

# üîπ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì AUC
auc_train = auc(fpr_train, tpr_train)
auc_test = auc(fpr_test, tpr_test)

# üîπ Plot ROC Curve
plt.figure(figsize=(8, 6))
plt.plot(fpr_train, tpr_train, label=f'Train ROC (AUC = {auc_train:.4f})', linestyle='--')
plt.plot(fpr_test, tpr_test, label=f'Test ROC (AUC = {auc_test:.4f})', color='darkorange')
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")  # ‡πÄ‡∏™‡πâ‡∏ô‡πÑ‡∏î‡∏≠‡∏≤‡∏Å‡∏≠‡∏ô‡∏±‡∏•
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend()
plt.grid(True)
plt.show()